# sample-dataverse-app

An Apache Spark application for OpenShift using Pyspark and Flask that binds to services provided by [dataverse-broker](https://github.com/dataverse-broker/dataverse-broker).

This project uses OpenShift's source-to-image tool.

## Quick start

You should have access to an OpenShift cluster and be logged in with the
`oc` command line tool. 

1. Select a Dataverse service from OpenShift's Service Catalog, and create a secret. If dataverse services aren't listed, contact your cluster admin. Or, if deploying to a local cluster, follow the instructions at the (dataverse-broker)[https://github.com/dataverse-broker/dataverse-broker] project.

2. Create the necessary infrastructure objects
   ```bash
   oc create -f https://radanalytics.io/resources.yaml
   ```

3. Launch spark app
   ```bash
   oc new-app --template oshinko-python-spark-build-dc  \
       -p APPLICATION_NAME=pyspark \
       -p GIT_URI=https://github.com/dataverse-broker/sample-dataverse-app
   ```

4. Add the secret created in step 1 to the pyspark application.

5. Expose an external route
   ```bash
   oc expose svc/pyspark
   ```

6. Visit the exposed URL with your browser or other HTTP tool, and be prompted by the web UI.

## Explanation of certain items

The `requirements.txt` file contains requirements which setups this web application to listen on port 8080 and run the `Spark` job, as well as stating certain depencies that our scripts utilize.

The `.s2i` folder contains the environment variables for our main script which is added to the source image that is generated by radanalytics.io / Oshinko

The `dataverse_lib.py` script contains the functions for interacting with the [`Dataverse API`](http://guides.dataverse.org/en/latest/api/), such as searching and downloading certain files within a dataverse, or a dataverse subtree.

The `spark_wordcount.py` script contains the logic for running the `Spark` word count file on a txt file.

The `main.py` script contains a `Flask` application which allows you to select a file from the Dataverse service to run through wordcount, and displays the results.


## Credits

This project is based off of [radanalytics.io](https://radanalytics.io)'s [tutorial-sparkpi-python-flask](https://github.com/radanalyticsio/tutorial-sparkpi-python-flask) project.

The `spark_wordcount.py` spark application is borrowed from [this StackOverflow answer](https://stackoverflow.com/a/32845282).

Calls to dataverse follow a mix of Search API, Data Access API, and Native API calls to Dataverse as documented [here](http://guides.dataverse.org/en/latest/api/).
