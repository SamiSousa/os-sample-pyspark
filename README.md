# os-sample-pyspark
A quick Apache Spark application for OpenShift in Python

This readme is based off of [radanalytics.io](https://radanalytics.io)'s [tutorial-sparkpi-python-flask](https://github.com/radanalyticsio/tutorial-sparkpi-python-flask) project

It is intended to be
used as a source-to-image application.

## Quick start

You should have access to an OpenShift cluster and be logged in with the
`oc` command line tool.

1. Create the necessary infrastructure objects
   ```bash
   oc create -f https://radanalytics.io/resources.yaml
   ```

2. Launch spark app
   ```bash
   oc new-app --template oshinko-python-spark-build-dc  \
       -p APPLICATION_NAME=pyspark \
       -p GIT_URI=https://github.com/SamiSousa/os-sample-pyspark
   ```

3. Expose an external route
   ```bash
   oc expose svc/pyspark
   ```

4. Visit the exposed URL with your browser or other HTTP tool, for example:
   ```bash
   $ curl http://`oc get routes/pyspark --template='{{.spec.host}}'`
   Python Flask Spark server running. Add the 'main' route to this URL to invoke the app.

   $ curl http://`oc get routes/pyspark --template='{{.spec.host}}'`/main
   (something from wordcount)
   ```

## Explanation of certain items

The `requirements.txt` file contains requirements which setups this web application to listen on port 8080 and run the `Spark` job, as well as stating certain depencies that our scripts utilize.

The `.s2i` folder contains the environment variables for our main script which is added to the source image that is generated by radanalytics.io / Oshinko

The `dataverse_lib.py` script contains the functions for interacting with the `Dataverse API`, such as searching and downloading certain files within a dataverse, or a dataverse subtree.

The `main.py` script contains the logic for running the `Spark` word count file on a txt file, and also contains a simple `Flask` implementation which displays the word count on a browser via localhost.


